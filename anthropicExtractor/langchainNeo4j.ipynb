{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_community langchain neo4j langchain-huggingface ipywidgets einops pypdf tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ollama_base_url = \"\"\n",
    "\n",
    "if os.getenv(\"BASE_URL\"):\n",
    "    ollama_base_url = os.getenv(\"BASE_URL\")\n",
    "else:\n",
    "    ollama_base_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFace embeddings model with specific parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\", \"trust_remote_code\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neo4j database connection parameters\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"yourpassword\"\n",
    "\n",
    "# Initialize the Neo4jGraph object with the connection parameters\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    ")\n",
    "\n",
    "# Create a Neo4jVector object from the existing graph database\n",
    "vecFromGraphDB = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=\"vecFromGraph\",\n",
    "    node_label=\"GovernmentDocument\",\n",
    "    text_node_properties=[\"summary\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")\n",
    "\n",
    "vector_db = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    persist_directory=\"./chromaVDB\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search on the vector database with a query\n",
    "question = \"What is the maximum percentage of any corporate bond issue that a single FPI can invest in?\"\n",
    "simResults = vecFromGraphDB.similarity_search_with_relevance_scores(question)\n",
    "print(simResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Cypher query to find nodes connected to a specific document\n",
    "connected_docs = []\n",
    "connected_docs.append(simResults[0][0].metadata[\"name\"])\n",
    "\n",
    "matched_docs = []\n",
    "\n",
    "while len(connected_docs) > 0:\n",
    "    for i in connected_docs:\n",
    "        query = f\"MATCH (start {{name: '{i}'}})<-[r]-(connected) RETURN connected\"\n",
    "        query_results = graph.query(query)\n",
    "\n",
    "        for record in query_results:\n",
    "            connected_docs.append(record[\"connected\"][\"name\"])\n",
    "\n",
    "        matched_docs.append(i)\n",
    "        connected_docs.remove(i)\n",
    "\n",
    "matched_docs = list(set(matched_docs))\n",
    "print(matched_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Cypher query to find nodes connected to a specific document\n",
    "matched_docs = []\n",
    "\n",
    "query = f\"MATCH (start {{name: '{simResults[0][0].metadata['name']}'}})<-[r]-(connected) RETURN connected\"\n",
    "query_results = graph.query(query)\n",
    "\n",
    "for record in query_results:\n",
    "    matched_docs.append(record[\"connected\"][\"name\"])\n",
    "\n",
    "for i in matched_docs:\n",
    "    query = f\"MATCH (start {{name: '{i}'}})<-[r]-(connected) RETURN connected\"\n",
    "    query_results = graph.query(query)\n",
    "\n",
    "    for record in query_results:\n",
    "        matched_docs.append(record[\"connected\"][\"name\"])\n",
    "\n",
    "matched_docs.append(simResults[0][0].metadata[\"name\"])\n",
    "\n",
    "matched_docs = list(set(matched_docs))\n",
    "print(matched_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=ollama_base_url,\n",
    "    model=\"llama3.1\",\n",
    "    format=\"json\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "     \n",
    "    <Retrieved Document> \\n\n",
    "    {document} \n",
    "    </Retrieved Document> \\n\n",
    "    \n",
    "\n",
    "    <User Question> \\n\n",
    "    {question}\n",
    "    </User Question> \\n\n",
    "    \n",
    "    If the document contains keywords related to the user question, grade it as relevant. Use logic and understand the context of the question and document to make decisions. Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in matched_docs:\n",
    "    ret_content = vector_db.similarity_search(question, filter={\"name\": doc}, k=1)\n",
    "    print(ret_content[0].page_content)\n",
    "    # response = retrieval_grader.invoke({\"question\": question, \"document\": doc})\n",
    "    # print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
